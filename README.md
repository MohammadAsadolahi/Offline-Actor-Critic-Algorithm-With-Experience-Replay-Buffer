# Offline-Actor-Critic-Algorithm-With-Experience-Replay-Buffer
Combining experience replay buffer whith Actor-Critic algorithm to expedite learning procedure by offline learning
This codde is a single Actor-critic Agent deployed along with a experience replay buffer which is comonly used in off-policy reinforcement algorithms  
**this project can be applied for !any on-policy Actor-Critic algorithm! to expedite learning procedure**   
**this project is a research project and owners have no responsibility of any loss for deploying this project in real environmentn**   

[feel free to ask any question in Issues or just email me]  
Mohammad.E.Asadolahi@gmail.com
  
  
**this code is not implemented with Pytorch! for TensorFlow version look up my Github repositories**
### to do:  
* write the ReplayBuffer code   [***done***]
* deploy Actor and Critic neural networks
* deploy the Agnet in pytorch 
* deploy the Environmetn class (using OpenAI Gym library) 
* deploy the main learning loop
* Deploy the test loop
